{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUb2oSz45XV4"
      },
      "source": [
        "# Fine-tuned LLM - PubMedQA\n",
        "\n",
        "---\n",
        "### Objective:\n",
        "Develop an LLM for a healthcare chatbot that answers patient inquiries on medications, symptoms, and  treatments while reducing hallucinations.\n",
        "\n",
        "### Datasets:\n",
        "* PubMedQA – Biomedical QA dataset - https://huggingface.co/datasets/qiaojin/PubMedQA\n",
        "\n",
        "### Base Model:\n",
        "• Llama-2 7B\n",
        "\n",
        "### Evaluation Metrics:\n",
        "* Rouge, BLEU, Meteor, BertScore, Manual inspection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C8_1rPk-OkG"
      },
      "source": [
        "---\n",
        "## 1. Import libraries and model to prepare for fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0NvJyuYeGBJ"
      },
      "outputs": [],
      "source": [
        "!pip install datasets requests bitsandbytes accelerate peft trl sentencepiece wandb transformers evaluate rouge_score bert-score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIeIbt2M9vut"
      },
      "source": [
        "### IMPORTANT: Restart Colab runtime after PIP install!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Implement base model and imports\n",
        "First step is to import all necessary libraries and implement base model, to validate it can generate a response from prompt."
      ],
      "metadata": {
        "id": "lxw-QITQ9iW0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqaBKB9e4Vhe"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    logging,\n",
        "    pipeline\n",
        ")\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model, TaskType\n",
        "from trl import SFTTrainer, SFTConfig, DataCollatorForCompletionOnlyLM\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from datetime import datetime\n",
        "import torch\n",
        "import wandb\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from bert_score import BERTScorer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4wnNrTyAJKX"
      },
      "outputs": [],
      "source": [
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "project_name = userdata.get(\"PROJECT_NAME\")\n",
        "hf_username = userdata.get(\"HF_USERNAME\")\n",
        "dataset_id = userdata.get(\"DATASET_ID\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8r6Ypk6_x0S"
      },
      "outputs": [],
      "source": [
        "# log into hugging face and wandb\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token)\n",
        "\n",
        "wandb_api_key = userdata.get('WANDB_API_KEY')\n",
        "os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n",
        "wandb.login()\n",
        "\n",
        "# Configure Weights & Biases to record against our project\n",
        "os.environ[\"WANDB_PROJECT\"] = userdata.get(\"PROJECT_NAME\")\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\" if True else \"end\"\n",
        "os.environ[\"WANDB_WATCH\"] = \"gradients\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcmAaL_04ONh"
      },
      "outputs": [],
      "source": [
        "# quantisation config to use less memory when loading model\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vCtSkep6wCb"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quant_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jj4U50PY7o0Z"
      },
      "outputs": [],
      "source": [
        "memory = model.get_memory_footprint() / 1e6\n",
        "print(f\"Memory footprint: {memory:,.1f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcZz_RQU-N0l"
      },
      "outputs": [],
      "source": [
        "# model architecture\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJK_xYZlCUbj"
      },
      "outputs": [],
      "source": [
        "# initialise tokenizer and pipeline\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALuROA3g6cPq"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful personalised medical assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"How can I get rid of the flu?\"}\n",
        "  ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U73PHhEU-ZaH"
      },
      "outputs": [],
      "source": [
        "# verify base model generates response\n",
        "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(inputs, max_new_tokens=250)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du1KeJ2moW0s"
      },
      "source": [
        "## 3. Dataset analysis and preprocessing for Llama\n",
        "The datasets need to be analysed and prepared for fine-tuning the model. This includes removing invalid data and formatting into the required format with special tokens the Llama 2 model requires. Dataset acquired from HuggingFace\n",
        "* PubMedQA - https://huggingface.co/datasets/qiaojin/PubMedQA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5U2t0GudQnjq"
      },
      "outputs": [],
      "source": [
        "# format input from datasets into prompt format required by llama model\n",
        "def format_llama_prompt(user_message, model_answer):\n",
        "  prompt = '<s>[INST] ' # special token - commence instruct\n",
        "  prompt += user_message.strip()\n",
        "  prompt += ' [/INST] ' # special token - end instruct\n",
        "  prompt += model_answer.strip()\n",
        "  prompt += ' </s>'# special token - end\n",
        "  return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tigpUr27IbZ"
      },
      "outputs": [],
      "source": [
        "# load pub_med_qa dataset\n",
        "pub_med_qa = load_dataset(\"qiaojin/PubMedQA\", \"pqa_artificial\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ND4-gHOdcnzp"
      },
      "outputs": [],
      "source": [
        "pub_med_qa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENO4TezgGW0T"
      },
      "outputs": [],
      "source": [
        "pub_med_qa['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0fWQxXlOW6A"
      },
      "outputs": [],
      "source": [
        "# create list of formatted prompts\n",
        "pub_med_qa_as_prompt = []\n",
        "for item in pub_med_qa['train']:\n",
        "  if item['final_decision'] == 'yes': # only get rows with yes decision\n",
        "    pub_med_qa_as_prompt.append(format_llama_prompt_with_context(item['question'], item['long_answer'], item['context']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coPjx5TLGwyn"
      },
      "outputs": [],
      "source": [
        "print(len(pub_med_qa_as_prompt))\n",
        "print(pub_med_qa_as_prompt[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAjqn2ffkRpg"
      },
      "outputs": [],
      "source": [
        "pub_med_qa_as_prompt[200]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ts8u9Dp2tlqu"
      },
      "outputs": [],
      "source": [
        "# collect all formatted data into one dataset\n",
        "prompt_dataset = pub_med_qa_as_prompt # + med_dialog_hcm_as_prompt + med_dialog_ic_as_prompt\n",
        "print(len(prompt_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjpG35k6TiiV"
      },
      "outputs": [],
      "source": [
        "# convert to huggingface dataset, create splits and push to hub\n",
        "prompt_hf_dataset = Dataset.from_dict({\"text\": prompt_dataset})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybrNf0dVVgXR"
      },
      "outputs": [],
      "source": [
        "ds_train = prompt_hf_dataset.train_test_split(test_size=0.2, seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ca4Mju9PVtr3"
      },
      "outputs": [],
      "source": [
        "ds_test = ds_train['test'].train_test_split(test_size=0.5, seed=42)\n",
        "ds_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Np2CXY-iPUil"
      },
      "outputs": [],
      "source": [
        "ds_splits = DatasetDict({\n",
        "    'train': ds_train['train'],\n",
        "    'validation': ds_test['train'],\n",
        "    'test': ds_test['test']\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4bWyjWeU_ut"
      },
      "outputs": [],
      "source": [
        "ds_splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-V9R707vR9HH"
      },
      "outputs": [],
      "source": [
        "# push to hugging face\n",
        "ds_splits.push_to_hub(f\"{hf_username}/{project_name}-pubmedqa-artifical-with-context\", private=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwQ0tJ6SpEhu"
      },
      "source": [
        "## 4. Model Training Pipeline\n",
        "The model training pipeline is setup for fine-tuning. During this process additional data processing was required to filter out data with execessive token length that would exhaust GPU resources.\n",
        "\n",
        "Initially four variants(A,B,C,D) of the model were fine-tuned on combination dataset, however focus is now on model variant E. It maintains integration with HuggingFace and Weight and Bias platform so models and runs are saved for later retrieval.\n",
        "\n",
        "The models were fine-tuned on the **train** set, and evaluated with **validation** set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIkMaVEF2fsq"
      },
      "outputs": [],
      "source": [
        "# load pre-processed dataset created in last step from hugging face\n",
        "qa_dataset = load_dataset(f\"{hf_username}/{project_name}-pubmedqa-artifical-with-context\") # load only pubmedqa YES rows\n",
        "qa_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fw4YL5RSz8Ue"
      },
      "outputs": [],
      "source": [
        "# getting token lengths from training dataset\n",
        "train_length = []\n",
        "train_token_length = []\n",
        "for item in qa_dataset['train']:\n",
        "  if item['text']:\n",
        "    train_length.append(len(item['text']))\n",
        "    tokens = tokenizer.encode(item['text'])\n",
        "    train_token_length.append(len(tokens))\n",
        "  else:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNjYg5Dj1k-J"
      },
      "outputs": [],
      "source": [
        "len(train_token_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsGzUkKO147u"
      },
      "outputs": [],
      "source": [
        "# print the min, max and average character lengths and token count lengths for training data\n",
        "print(min(train_length), max(train_length), sum(train_length)/len(train_length))\n",
        "print(min(train_token_length), max(train_token_length), sum(train_token_length)/len(train_token_length))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSy_f8UQcKOO"
      },
      "outputs": [],
      "source": [
        "# plot the token lengths\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.hist(train_token_length, rwidth=0.7, bins=100)\n",
        "plt.xlabel('Token Length')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Token Length Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yh9GxruEX-Zn"
      },
      "outputs": [],
      "source": [
        "# avg is now approx 104, but will keep 230 tokens, therefore will create new datasets\n",
        "# to filter out any rows above this token count\n",
        "MAX_TOKENS = 250\n",
        "DATASET_SIZE = 1000\n",
        "def get_filtered_dataset(dataset, max_tokens, size):\n",
        "  count = 0\n",
        "  train_size = round(size * 0.9) # 90% for train\n",
        "  valid_size = round(size * 0.1) # 10% for eval\n",
        "  print(f\"Train size: {train_size}, Eval size: {valid_size}\")\n",
        "\n",
        "  filtered_train_dataset = []\n",
        "  for item in dataset['train']:\n",
        "    if item['text']:\n",
        "      tokens = tokenizer.encode(item['text'])\n",
        "      if len(tokens) < max_tokens:\n",
        "        filtered_train_dataset.append(item)\n",
        "        count += 1\n",
        "      if count >= train_size:\n",
        "        break\n",
        "\n",
        "  count = 0\n",
        "  filtered_eval_dataset = []\n",
        "  for item in dataset['validation']:\n",
        "    if item['text']:\n",
        "      tokens = tokenizer.encode(item['text'])\n",
        "      if len(tokens) < max_tokens:\n",
        "        filtered_eval_dataset.append(item)\n",
        "\n",
        "        count += 1\n",
        "      if count >= valid_size:\n",
        "        break\n",
        "\n",
        "  return filtered_train_dataset, filtered_eval_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sa8GkjdgTc5"
      },
      "outputs": [],
      "source": [
        "qa_train, qa_val = get_filtered_dataset(qa_dataset, MAX_TOKENS, DATASET_SIZE)\n",
        "qa_train = Dataset.from_list(qa_train)\n",
        "qa_val = Dataset.from_list(qa_val)\n",
        "print(qa_train)\n",
        "print(qa_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IAWIFt_6nya"
      },
      "outputs": [],
      "source": [
        "# set constants\n",
        "MAX_SEQUENCE_LENGTH = 230 # calculated from avg token lenght in dataset\n",
        "\n",
        "# Run name for saving the model in the hub\n",
        "RUN_NAME =  f\"{datetime.now():%Y-%m-%d_%H.%M.%S}\"\n",
        "PROJECT_RUN_NAME = f\"{project_name}-{RUN_NAME}\"\n",
        "HUB_MODEL_NAME = f\"{hf_username}/{PROJECT_RUN_NAME}\"\n",
        "\n",
        "# qlora hyper params\n",
        "LORA_R = 16 # Reduce LoRA rank (lower = less memory) , initial = 32\n",
        "LORA_ALPHA = 32 # Lower alpha , initial = 64\n",
        "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
        "LORA_DROPOUT = 0.05\n",
        "QUANT_4_BIT = True\n",
        "\n",
        "# training hyper params\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 4  # Reduce batch size , initial = 4\n",
        "GRADIENT_ACCUMULATION_STEPS = 8  # Simulate batch size 8, initial = 1\n",
        "LEARNING_RATE = 2e-4\n",
        "LR_SCHEDULER_TYPE = 'cosine'\n",
        "WARMUP_RATIO = 0.03\n",
        "OPTIMIZER = \"paged_adamw_32bit\"\n",
        "STEPS = 50\n",
        "SAVE_STEPS = 2000\n",
        "LOG_TO_WANDB = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wJwH1MA-tyo"
      },
      "outputs": [],
      "source": [
        "response_template = \" [/INST] \"\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbo_-tkbpICk"
      },
      "outputs": [],
      "source": [
        "lora_params = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    target_modules=TARGET_MODULES,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_WWBQVSyPJu"
      },
      "outputs": [],
      "source": [
        "training_params = SFTConfig(\n",
        "    output_dir=PROJECT_RUN_NAME,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=1,\n",
        "    #eval_strategy=\"no\",\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    optim=OPTIMIZER,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_total_limit=10,\n",
        "    logging_steps=STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.001,\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
        "    report_to=\"wandb\" if True else None,\n",
        "    run_name=RUN_NAME,\n",
        "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
        "    dataset_text_field=\"text\",\n",
        "    save_strategy=\"steps\",\n",
        "    hub_strategy=\"every_save\",\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=HUB_MODEL_NAME,\n",
        "    hub_private_repo=True\n",
        "    #neftune_noise_alpha=5 # using NEFTune as describe in SFT Trainer docs for increased conversational quality\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaOIGnQ4jxVS"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsMERCCJj99j"
      },
      "outputs": [],
      "source": [
        "# clear gpu memory\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_max_memory_allocated()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the model for k-bit training (important for 4-bit)\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "lz825BwicfAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_peft_model(model, lora_params)"
      ],
      "metadata": {
        "id": "pp8vo6najkmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NY_UzTxetRc5"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=qa_train,\n",
        "    eval_dataset=qa_val,\n",
        "    peft_config=lora_params,\n",
        "    args=training_params\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6Ko4tyGJftW_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ushOfJdwuMAZ"
      },
      "outputs": [],
      "source": [
        "#torch.cuda.empty_cache()\n",
        "trainer.train()\n",
        "trainer.model.push_to_hub(PROJECT_RUN_NAME, private=True)\n",
        "print(f\"Model saved to HuggingFace as: {PROJECT_RUN_NAME}\")\n",
        "model.save_pretrained(PROJECT_RUN_NAME)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7QAra8y9v2Z"
      },
      "outputs": [],
      "source": [
        "results = trainer.evaluate()\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "IGIlJtkHn3Ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stop notebook and disconnect GPU after finishing above steps as this process can take several hours\n",
        "# from google.colab import runtime\n",
        "# runtime.unassign()"
      ],
      "metadata": {
        "id": "RAC0SrTL8Wt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6sQTtZuSoT0"
      },
      "source": [
        "## 5. Evaluate the models - generate evaluation data\n",
        "At this step the fine-tuned model created in the previous step can be re-loaded without need to re-run step 4. Generated responses will be collected, in addition to the base model to use as a benchmark.\n",
        "\n",
        "The prompts to generate the responses are 50 samples from the **test** set, which was not used for fine-tuning.\n",
        "\n",
        "The generated response for each model are then saved to HuggingFace hub for later analysis."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define models saved to huggingface hub from previous step\n",
        "MODEL_G = f\"{hf_username}/{project_name}-2025-05-08_01.23.37\" # trained on 9000 rows"
      ],
      "metadata": {
        "id": "BsSyJO-eOIbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quantisation config to use less memory when loading model\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ],
      "metadata": {
        "id": "ibdFI1MXDOAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_base = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\")"
      ],
      "metadata": {
        "id": "9SZwXR5wOB8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialise tokenizer and pipeline\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "7f062UDIQcVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfqi9mxLcCV_"
      },
      "outputs": [],
      "source": [
        "qa_test = load_dataset(f\"{hf_username}/{project_name}-pubmedqa-artifical\", split=\"test[:50]\") # load only pubmedqa YES rows, test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ex4akcIcHh4"
      },
      "outputs": [],
      "source": [
        "# split into question and answer list\n",
        "qa_list = []\n",
        "for item in qa_test:\n",
        "  prompt = item['text']\n",
        "  question = re.search(r'\\[INST\\] (.*) \\[/INST\\]', prompt).group(1)\n",
        "  response = re.search(r'\\[/INST\\] (.*) \\</s\\>', prompt).group(1)\n",
        "  qa_list.append([question, response])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smzkASMLcfeK"
      },
      "outputs": [],
      "source": [
        "qa_list[20][0] # question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aHb4x2hfZFi"
      },
      "outputs": [],
      "source": [
        "prompt1 = \"You are a helpful personalised medical assistant. In your response do not include any personal names and keep your answer professional and concise.\"\n",
        "#prompt2 = \"You are a helpful personalised medical assistant. Provide a friendly personal response to the patients medical question within a paragraph, provide the necessary details. Follow up it they need more information\"\n",
        "\n",
        "def get_model_responses(model_x, qa_list):\n",
        "  model_responses = []\n",
        "  for item in qa_list:\n",
        "    question = item[0]\n",
        "    # print(item)\n",
        "    # print(question)\n",
        "    # print(\"-----\")\n",
        "    messages = [\n",
        "      {\"role\": \"system\", \"content\": prompt1},\n",
        "      {\"role\": \"user\", \"content\": question}\n",
        "    ]\n",
        "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model_x.generate(inputs, max_new_tokens=250, temperature=0.1)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "    answer = response.split('[/INST] ')[1]  # get text after /INST token\n",
        "    answer = answer.replace('</s>', '') # remove trailing special token if present\n",
        "    model_responses.append(answer)\n",
        "  return model_responses"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_base.lm_head.weight[0, :10])\n",
        "print(model_g.lm_head.weight[0, :10])"
      ],
      "metadata": {
        "id": "yFvSVzQOFUwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_base_responses_pubmedqa = get_model_responses(model_base, qa_list)"
      ],
      "metadata": {
        "id": "sX8ZCqBJ0JmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_base_pubmedqa_dataset = Dataset.from_dict({\"text\": model_base_responses_pubmedqa})"
      ],
      "metadata": {
        "id": "JPAzLIad0h1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_base_pubmedqa_dataset.push_to_hub(f\"{hf_username}/model_base_responses_pubmedqa\", private=True)"
      ],
      "metadata": {
        "id": "2n-XKStD1Bzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_base_pubmedqa_dataset[1]"
      ],
      "metadata": {
        "id": "K3fR3LDm1Tr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_g = PeftModel.from_pretrained(model_base, MODEL_G)"
      ],
      "metadata": {
        "id": "1Ya_zfN6ylgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_g_responses = get_model_responses(model_g, qa_list)\n",
        "prompt_g_dataset = Dataset.from_dict({\"text\": model_g_responses})\n",
        "prompt_g_dataset.push_to_hub(f\"{hf_username}/model_g_responses\", private=True)"
      ],
      "metadata": {
        "id": "nWoapdbM-9tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stop notebook and disconnect GPU after finishing above steps as this process can take several hours\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "Ub6-RR3uTnVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Evaluate fine tuned models - get metrics from generated responses\n",
        "In this final step, the generated response can be reloaded from HuggingFace to evaluate and calculate metrics against the expected responses in **test** dataset."
      ],
      "metadata": {
        "id": "KwqCV9psQNpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_test = load_dataset(f\"{hf_username}/{project_name}-pubmedqa-artifical\", split=\"test[:50]\") # load only pubmedqa YES rows, test set"
      ],
      "metadata": {
        "id": "gfyOLbFnQVqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_list = []\n",
        "for item in qa_test:\n",
        "  prompt = item['text']\n",
        "  question = re.search(r'\\[INST\\] (.*) \\[/INST\\]', prompt).group(1)\n",
        "  response = re.search(r'\\[/INST\\] (.*) \\</s\\>', prompt).group(1)\n",
        "  qa_list.append([question, response])"
      ],
      "metadata": {
        "id": "cY3cjfLbQsJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_list[20][1] # index 1 is dataset real response"
      ],
      "metadata": {
        "id": "mAGTjeruQwJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_references = [item[1] for item in qa_list] # get list of reference responses"
      ],
      "metadata": {
        "id": "HVQzvtF1VIz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load saved response data to evaluate\n",
        "model_base_pubmedqa_dataset = load_dataset(f\"{hf_username}/model_base_responses_pubmedqa\")\n",
        "model_g_dataset = load_dataset(f\"{hf_username}/model_g_responses\")"
      ],
      "metadata": {
        "id": "526aY-AERByq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to list\n",
        "base_pubmedqa_response_list = model_base_pubmedqa_dataset['train']['text'][:50]\n",
        "g_response_list = model_g_dataset['train']['text']"
      ],
      "metadata": {
        "id": "OA2TvC2CR9an"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_rouge_metric(references, predictions):\n",
        "  rouge = evaluate.load('rouge')\n",
        "  results = rouge.compute(\n",
        "    predictions=predictions,\n",
        "    references=references\n",
        "  )\n",
        "  return results\n",
        "\n",
        "def calc_bleu_metric(references, predictions):\n",
        "  bleu = evaluate.load('bleu')\n",
        "  results = bleu.compute(\n",
        "    predictions=predictions,\n",
        "    references=references\n",
        "  )\n",
        "  return results\n",
        "\n",
        "def calc_meteor_metric(references, predictions):\n",
        "  meteor = evaluate.load('meteor')\n",
        "  results = meteor.compute(\n",
        "    predictions=predictions,\n",
        "    references=[[ref] for ref in references]\n",
        "  )\n",
        "  return results\n",
        "\n",
        "def calc_bert_score(references, predictions):\n",
        "  bert_scorer = BERTScorer(model_type='bert-base-uncased')\n",
        "  P = []\n",
        "  R = []\n",
        "  F1 = []\n",
        "\n",
        "  for i in range(len(predictions)):\n",
        "    p_i, r_i, f1_i = bert_scorer.score([predictions[i]], [references[i]])\n",
        "    P.append(p_i)\n",
        "    R.append(r_i)\n",
        "    F1.append(f1_i)\n",
        "  results = {\n",
        "    'P': np.mean(P),\n",
        "    'R': np.mean(R),\n",
        "    'F1': np.mean(F1)\n",
        "  }\n",
        "  return results"
      ],
      "metadata": {
        "id": "yzfSECeMTK67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_base_pubmedqa = calc_rouge_metric(qa_references, base_pubmedqa_response_list)\n",
        "rouge_g = calc_rouge_metric(qa_references, g_response_list)"
      ],
      "metadata": {
        "id": "7CnxpXVMU-kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rouge_base_pubmedqa)\n",
        "print(rouge_g)"
      ],
      "metadata": {
        "id": "R2dEf1Mw5tbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot rouge scores\n",
        "rouge_dict = {\n",
        "    'rouge1': (rouge_base_pubmedqa['rouge1'], rouge_g['rouge1']),\n",
        "    'rouge2': (rouge_base_pubmedqa['rouge2'], rouge_g['rouge2']),\n",
        "    'rougeL': (rouge_base_pubmedqa['rougeL'], rouge_g['rougeL']),\n",
        "    'rougeLsum': (rouge_base_pubmedqa['rougeLsum'], rouge_g['rougeLsum'])\n",
        "}\n",
        "model_labels = ('Base', 'Model G')\n",
        "\n",
        "x = np.arange(len(model_labels))\n",
        "width = 0.2\n",
        "multiplier = 0\n",
        "\n",
        "fig, ax = plt.subplots(layout='constrained')\n",
        "\n",
        "for attribute, measurement in rouge_dict.items():\n",
        "    offset = width * multiplier\n",
        "    msr = np.round(measurement, 4) # round to 4 decimal places for plot\n",
        "    rects = ax.bar(x + offset, msr, width, label=attribute)\n",
        "    ax.bar_label(rects, padding=1)\n",
        "    multiplier += 1\n",
        "\n",
        "ax.set_ylabel('Rouge Score')\n",
        "ax.set_title('Rouge Score Comparison')\n",
        "ax.set_xticks(x + width, model_labels)\n",
        "ax.legend(loc='upper left', ncols=4)\n",
        "ax.set_ylim(0, 1)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n6rBYsoNN4_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_base_pubmedqa = calc_bleu_metric(qa_references, base_pubmedqa_response_list)\n",
        "bleu_g = calc_bleu_metric(qa_references, g_response_list)"
      ],
      "metadata": {
        "id": "LvagAtBTTJam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(bleu_base_pubmedqa)\n",
        "print(bleu_g)"
      ],
      "metadata": {
        "id": "E_P5x81pTTq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot bleu scores\n",
        "bleu_dict = {\n",
        "    'bleu': (bleu_base_pubmedqa['bleu'], bleu_g['bleu']),\n",
        "    'brevity_penalty': (bleu_base_pubmedqa['brevity_penalty'], bleu_g['brevity_penalty'])\n",
        "}\n",
        "\n",
        "model_labels = ('Base', 'Model G')\n",
        "\n",
        "x = np.arange(len(model_labels))\n",
        "width = 0.2\n",
        "multiplier = 0\n",
        "\n",
        "fig, ax = plt.subplots(layout='constrained')\n",
        "\n",
        "for attribute, measurement in bleu_dict.items():\n",
        "    offset = width * multiplier\n",
        "    msr = np.round(measurement, 4) # round to 4 decimal places for plot\n",
        "    rects = ax.bar(x + offset, msr, width, label=attribute)\n",
        "    ax.bar_label(rects, padding=1)\n",
        "    multiplier += 1\n",
        "\n",
        "ax.set_ylabel('Bleu Score')\n",
        "ax.set_title('Bleu Score Comparison')\n",
        "ax.set_xticks(x + width, model_labels)\n",
        "ax.legend(loc='upper left', ncols=4)\n",
        "ax.set_ylim(0, 1)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VrdOyoXrUojV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meteor_base_pubmedqa = calc_meteor_metric(qa_references, base_pubmedqa_response_list)\n",
        "meteor_g = calc_meteor_metric(qa_references, g_response_list)"
      ],
      "metadata": {
        "id": "c1Hq-mzZTiSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(meteor_base_pubmedqa)\n",
        "print(meteor_g)"
      ],
      "metadata": {
        "id": "DjJmRWA6T5Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot meteor scores\n",
        "meteor_dict = {\n",
        "    'meteor': (meteor_base_pubmedqa['meteor'], meteor_g['meteor'])\n",
        "}\n",
        "model_labels = ('Base', 'Model G')\n",
        "\n",
        "x = np.arange(len(model_labels))\n",
        "width = 0.2\n",
        "multiplier = 0\n",
        "\n",
        "fig, ax = plt.subplots(layout='constrained')\n",
        "\n",
        "for attribute, measurement in meteor_dict.items():\n",
        "    offset = width * multiplier\n",
        "    msr = np.round(measurement, 4) # round to 4 decimal places for plot\n",
        "    rects = ax.bar(x + offset, msr, width, label=attribute)\n",
        "    ax.bar_label(rects, padding=1)\n",
        "    multiplier += 1\n",
        "\n",
        "ax.set_ylabel('Meteor Score')\n",
        "ax.set_title('Meteor Score Comparison')\n",
        "ax.set_xticks(x + width, model_labels)\n",
        "ax.legend(loc='upper left', ncols=4)\n",
        "ax.set_ylim(0, 1)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p0MZQuIYW738"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_score_base_pubmedqa = calc_bert_score(qa_references, base_pubmedqa_response_list)\n",
        "bert_score_g = calc_bert_score(qa_references, g_response_list)"
      ],
      "metadata": {
        "id": "PX2qUlOaUBBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(bert_score_base_pubmedqa)\n",
        "print(bert_score_g)"
      ],
      "metadata": {
        "id": "tM-k91NhUM7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot bert scores\n",
        "bert_score_dict = {\n",
        "    'P': (bert_score_base_pubmedqa['P'], bert_score_g['P']),\n",
        "    'R': (bert_score_base_pubmedqa['R'], bert_score_g['R']),\n",
        "    'F1': (bert_score_base_pubmedqa['F1'], bert_score_g['F1'])\n",
        "}\n",
        "model_labels = ('Base', 'Model G')\n",
        "\n",
        "x = np.arange(len(model_labels))\n",
        "width = 0.2\n",
        "multiplier = 0\n",
        "\n",
        "fig, ax = plt.subplots(layout='constrained')\n",
        "\n",
        "for attribute, measurement in bert_score_dict.items():\n",
        "    offset = width * multiplier\n",
        "    msr = np.round(measurement, 4) # round to 4 decimal places for plot\n",
        "    rects = ax.bar(x + offset, msr, width, label=attribute)\n",
        "    ax.bar_label(rects, padding=1)\n",
        "    multiplier += 1\n",
        "\n",
        "ax.set_ylabel('Bert Score')\n",
        "ax.set_title('Bert Score Comparison')\n",
        "ax.set_xticks(x + width, model_labels)\n",
        "ax.legend(loc='upper left', ncols=4)\n",
        "ax.set_ylim(0, 1)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rKJ15yvHYFoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# human evaluation\n",
        "def print_response(QA_NUM):\n",
        "  for i in range(QA_NUM):\n",
        "    print('-----------------------------------------')\n",
        "    print('\\nTest question: ' + qa_list[i][0])\n",
        "    print('\\nTest answer: ' + qa_list[i][1])\n",
        "    print('\\nBase: ' + base_pubmedqa_response_list[i])\n",
        "    print('\\nModel G: ' + g_response_list[i])\n",
        "\n",
        "print_response(5)"
      ],
      "metadata": {
        "id": "ilTsBxeBAFD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Evaluation Summary\n",
        "Evaluation metrics do show some improvement over base model. The Llama 2 model does not disclose its training dataset, however as PubMedQA is public data, there exists the possiblity that it may have already been trained on this dataset.\n",
        "\n",
        "Alternatively, the training hyperparameters used in training and/or the volume of data used may have been insufficinet for the model to successfully learn the domain.\n",
        "\n",
        "Next steps:\n",
        "* Continue to experiment with further hyperparameter tuning\n",
        "* Increase volume of training data\n",
        "* Adjust system prompt, and model parameters on inference\n",
        "* Explore different dataset"
      ],
      "metadata": {
        "id": "A__kV9qaZ-lF"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Kh2len0SEWYqzObuw3YszHRIkeLe1zts",
      "authorship_tag": "ABX9TyPjvJUFotgpY0QFMNxw54Ux"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}